{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Top'></a><br><br>\n",
    "[Imports](#imports)<br> [Function List](#funclist)<br> [Apply band pass filter](#bpf)<br> [Create epochs using events](#epoching)<br> [Generate labels array from epochs](#labeling)<br>[Fit ICA to the epochs data](#ica)<br>[Apply ICA to the epochs data](#ica-apply)<br> [Apply standard scala](#sc)<br> [SVM](#svm)<br>[Model CNN](#createmodelCNN)<br>[Model CNN EEG-Conv-R](#createmodelCNN-R)<br>[Model RNN](#createmodelRNN)<br>[Model RNN convR](#createmodelRNNR)<br>[Confusion Matrix](#Confusionmarix)<br>[Define file paths](#filepaths)<br> [Load model saved in json file](#loadmodel)<br>[Fit the model](#fitmodel)<br>[Prediction](#prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "from mne.preprocessing import create_eog_epochs, create_ecg_epochs\n",
    "from mne.decoding import Scaler\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Add,Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D,Input,LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm \n",
    "\n",
    "import os.path \n",
    "from os import path\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='funclist'></a>\n",
    "# Function list\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_montage1 = r\"path1",
    "path_montage2 = r\"path2",
    "\n",
    "path_list_montage = [path_montage1, path_montage2]\n",
    "\n",
    "def loadfiles(path):\n",
    "    import glob\n",
    "    file_list = glob.glob(path)\n",
    "    return file_list\n",
    "\n",
    "    \n",
    "def readcnt_noevent(cnt_file, event_id, duration):\n",
    "    \n",
    "    for montage_path in path_list_montage:\n",
    "        if (path.exists(montage_path)):\n",
    "            montage = mne.channels.read_montage('NuAmps40', path=montage_path)\n",
    "    \n",
    "    \n",
    "    raw = mne.io.read_raw_cnt(cnt_file, montage, preload=True, verbose=0)\n",
    "\n",
    "    # drop unwanted channels \n",
    "\n",
    "\n",
    "    raw = raw.drop_channels(['FP2', 'F3', 'F4', 'FT7', 'FCZ', 'FT8', 'C3', 'C4', 'TP7', 'CPZ', 'TP8',\n",
    "                             'T5', 'PZ', 'T6', 'O1', 'O2', 'FT10', 'PO2'])\n",
    "\n",
    "    raw = raw.drop_channels(['HEOR','VEOL'])  \n",
    "\n",
    "    mne.io.set_bipolar_reference(raw, 'HEOL', 'VEOU', ch_name='EOG', copy=False)\n",
    "    \n",
    "    #raw.info['bads'] = ['F7', 'FZ', 'F8', 'FC4', 'A2']\n",
    "\n",
    "     \n",
    "    ###### add stimulus channel #########\n",
    "    stim_data = np.zeros((1, len(raw.times)))\n",
    "    info = mne.create_info(['STI'], raw.info['sfreq'], ['stim'])\n",
    "    stim_raw = mne.io.RawArray(stim_data, info)\n",
    "    raw.add_channels([stim_raw], force_update_info=True)\n",
    "\n",
    "    raw.set_channel_types({'STI': 'stim'})\n",
    "    \n",
    "    ##### Setting fixed length events ########\n",
    "    events = mne.make_fixed_length_events(raw, event_id , duration=duration)\n",
    "    \n",
    "    # Replacing the stim channel with events\n",
    "    channel_index_no = len(raw.ch_names)-1\n",
    "\n",
    "    for sample_index in events[:,0]:\n",
    "        #print(sample_index)\n",
    "        raw._data[channel_index_no, sample_index] = event_id\n",
    "        \n",
    "   \n",
    "    return raw;\n",
    "\n",
    "def raw_concat(raw1, raw2):\n",
    "    # Concatenating raws\n",
    "    from mne.io import  concatenate_raws\n",
    "    if(raw1 == None):\n",
    "        raw = raw2\n",
    "    else:\n",
    "        raws = [raw1, raw2]\n",
    "        raw = concatenate_raws(raws)\n",
    "    return raw;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save raw data into a single file\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training sample path for path 1 \n",
    "#path = training_data_path_home\n",
    "\n",
    "# training sample path for path 2\n",
    "#path = training_data_path_work \n",
    "\n",
    "def save_raw_data_single_file(folder_list):\n",
    "\n",
    "    event_id = None\n",
    "    raw = None\n",
    "\n",
    "    for folder in folder_list:\n",
    "        #print(folder)\n",
    "        file_list = loadfiles(folder+\"/*\")\n",
    "        for cnt_file in file_list:\n",
    "           \n",
    "            duration= 5\n",
    "            # raw.info\n",
    "\n",
    "            if(os.path.basename(cnt_file) == r\"Fatigue state.cnt\"):\n",
    "                event_id = 1\n",
    "\n",
    "            if(os.path.basename(cnt_file) == r\"Normal state.cnt\"):\n",
    "                event_id = 2\n",
    "\n",
    "            # Read the file 1\n",
    "            \n",
    "            raw1 = readcnt_noevent(cnt_file, event_id, duration)\n",
    "\n",
    "            # Concatenating files\n",
    "            raw = raw_concat(raw, raw1)\n",
    "\n",
    "    outfile = open(output_file_path + r'\\raw_data','wb')\n",
    "    pickle.dump(raw,outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read raw data file\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_raw_data_file(path):\n",
    "    inputfile = open(path + r\"\\raw_data\",\"rb\")\n",
    "    raw = pickle.load(inputfile)\n",
    "    return raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bpf'></a>\n",
    "# Apply band pass filter\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_pass_filter(raw):\n",
    "    raw.filter(4., 40., fir_design='firwin')\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='epoching'></a>\n",
    "# Create epochs using events\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoching(raw, event_ids):\n",
    "    \n",
    "    picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=[])\n",
    "    \n",
    "    events = mne.find_events(raw, stim_channel='STI', shortest_event=1)\n",
    "    \n",
    "    print('raw: ', raw.ch_names)\n",
    "    \n",
    "    # define an epoch size\n",
    "    tmin, tmax = -0.0, 0.499\n",
    "\n",
    "    # reject epochs which have their values over 150 micro volts\n",
    "    reject = None#dict(eeg=150e-6)\n",
    "\n",
    "    # create epochs\n",
    "    epochs = mne.Epochs(raw, events, event_id=event_ids, baseline=(0,0), tmin=tmin, tmax=tmax, \n",
    "                        preload=True, reject=reject, picks=picks)\n",
    "    #evoked = epochs.average()\n",
    "    print('epochs: ', epochs.ch_names)\n",
    "    epochs.plot(title='original')\n",
    "    \n",
    "\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='labeling'></a>\n",
    "# Generate labels array from epochs \n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(epochs):\n",
    "    # this list stores label of each epoch\n",
    "    epochs_labels = []\n",
    "\n",
    "    i = 0\n",
    "    epochs_length = epochs.__len__()\n",
    "    while i < epochs_length:\n",
    "        full_label_id = epochs[i].event_id\n",
    "        #print(full_label_id)\n",
    "        if ('1' in full_label_id.keys()):\n",
    "            epochs_labels.append(1)\n",
    "        elif ('fatigue_state' in full_label_id.keys()) :\n",
    "            epochs_labels.append(1)\n",
    "        else:\n",
    "            epochs_labels.append(0)\n",
    "        i = i+1\n",
    "     \n",
    "    # label format suitable for the 'loss' function\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    categoric_epochs_labels = to_categorical(epochs_labels, num_classes=2)\n",
    "    \n",
    "    print(categoric_epochs_labels[1], epochs_labels[1])\n",
    "        \n",
    "    return categoric_epochs_labels, epochs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ica'></a>\n",
    "# Fit ICA to the epochs data\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ica_preprocessing(epochs):\n",
    "    %matplotlib\n",
    "    \n",
    "    picks_eeg = mne.pick_types(epochs.info, meg=False, eeg=True, eog=False,\n",
    "                           stim=False, exclude='bads')\n",
    "    \n",
    "    #n_components = 25  # if float, select n_components by explained variance of PCA\n",
    "    method = 'fastica'  \n",
    "    decim = 3  \n",
    "\n",
    "    random_state = 23\n",
    "    \n",
    "    reject = dict(eeg=200e-6)\n",
    "    \n",
    "    ica = ICA(method=method, random_state=random_state)\n",
    "    \n",
    "    ica.fit(epochs.drop_bad(reject=reject), decim=decim)\n",
    "    \n",
    "\n",
    "    \n",
    "    return epochs,ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ica-apply'></a>\n",
    "# Apply ICA\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ica(ica_epochs):\n",
    "    eog_average = ica_epochs.average()\n",
    "    eog_inds, scores = ica.find_bads_eog(ica_epochs, ch_name='EOG' ) \n",
    "\n",
    "    ica.plot_scores(scores, exclude=eog_inds) \n",
    "\n",
    "\n",
    "    ica.plot_sources(eog_average, exclude=eog_inds) \n",
    "\n",
    "    if len(eog_inds) !=0 :\n",
    "        ica.plot_properties(ica_epochs, picks=eog_inds, psd_args={'fmax': 35.},\n",
    "                    image_args={'sigma': 1.})\n",
    "\n",
    "   \n",
    "\n",
    "    ica.plot_overlay(eog_average, exclude=eog_inds, show=False)\n",
    "\n",
    "    ica.exclude.extend(eog_inds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sc'></a>\n",
    "# Apply standard scalar\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(epochs, labels=None):\n",
    "    epochs_data = epochs.get_data()\n",
    "    \n",
    "    print(epochs_data.shape)\n",
    "    \n",
    "    h = epochs_data.shape[1]-1\n",
    "    w = epochs_data.shape[2]\n",
    "\n",
    "    # Remove EOG channel and reshape to fit the tensorflow input\n",
    "    epochs_data = epochs_data [:, 1:,:]\n",
    "    \n",
    "    print(epochs_data.shape)\n",
    "    \n",
    "    scaler = Scaler(info=None, scalings='mean')\n",
    "    epochs_scaled = scaler.fit_transform(epochs_data,labels)\n",
    "    print(epochs_scaled.shape)\n",
    "    \n",
    "    epochs_scaled = epochs_scaled.reshape(-1,h,w,1)\n",
    "    \n",
    " \n",
    "    return epochs_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>\n",
    "# SVM\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accept 2D array and labels\n",
    "\n",
    "def SVM_Classify(feature_vectors_final, labels):\n",
    "    #print(feature_vectors_final.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_vectors_final, labels, test_size=0.4, random_state=0);\n",
    "\n",
    "\n",
    "    clf = svm.SVC(kernel='rbf', C=2).fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(\"Accuracy is %f\" %clf.score(X_test, y_test));\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='createmodelCNN'></a>\n",
    "# CNN model layout\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_model(input_shape):\n",
    "\n",
    "    # Always have to give a 4D array as input to the CNN\n",
    "    visible = Input(input_shape[1:])\n",
    "\n",
    "    conv1 = Conv2D(32, (5,5), activation='relu', strides=1, bias_initializer='zeros')(visible)\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "    lrn2 = tf.nn.local_response_normalization(conv1)\n",
    "\n",
    "    conv3 = Conv2D(64, (3,3), strides=1, activation='relu', bias_initializer='zeros')(lrn2)\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2), padding='same',))\n",
    "\n",
    "    conv4 = Conv2D(32, (3,3), activation='relu', strides=1, bias_initializer='zeros')(conv3)\n",
    "\n",
    "    pool5 = MaxPooling2D(pool_size=(2,2), strides = 2)(conv4)\n",
    "\n",
    "    #flat1 = Flatten()(pool5)\n",
    "    dense1 = Dense(2048)(pool5)\n",
    "    fc6 = Dropout(0.5)(dense1)\n",
    "    flat1 = Flatten()(fc6)\n",
    "\n",
    "    output = Dense(2, activation='softmax')(flat1)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=r'model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='createmodelCNN-R'></a>\n",
    "# CNN EEG-Conv-R model\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X):\n",
    "\n",
    "    # Save the input value. need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    \n",
    "    \n",
    "    X = Conv2D(32, (5,5), strides=1, activation='relu', bias_initializer='zeros', padding = 'same')(X)\n",
    "    activation1 = Activation('relu')(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    \n",
    "    \n",
    "    X = Conv2D(32, (3,3), strides=1, bias_initializer='zeros', padding = 'same')(X)\n",
    "\n",
    "    \n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_model_R(input_shape):\n",
    "\n",
    "    # Always have to give a 4D array as input to the CNN\n",
    "    visible = Input(input_shape[1:])\n",
    "\n",
    "    X = Conv2D(32, (3,3), activation='relu', strides=3, bias_initializer='zeros', padding = 'same')(visible)\n",
    "    #print(X.shape)\n",
    "    \n",
    "    X = identity_block(X)\n",
    "    \n",
    "    lrn = tf.nn.local_response_normalization(X)\n",
    "\n",
    "    X = identity_block(lrn)\n",
    "\n",
    "    #pool5 = MaxPooling2D(pool_size=(2,2), strides = 2)(conv4)\n",
    "\n",
    "    dense = Dense(4096)(X)\n",
    "    fc = Dropout(0.5)(dense)\n",
    "    flat = Flatten()(fc)\n",
    "\n",
    "    output = Dense(2, activation='softmax')(flat)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    #print(model.summary())\n",
    "    plot_model(model, to_file=r'model_plot_R.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='createmodelRNN'></a>\n",
    "# RNN model layout\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(input_shape):\n",
    "    \n",
    "    #model = Sequential()\n",
    "    \n",
    "    # The input to every LSTM layer must be three-dimensional.\n",
    "    visible = Input(input_shape[1:-1])\n",
    "    \n",
    "    lstm1 = LSTM(512, return_sequences=True)(visible)\n",
    "    \n",
    "    lstm2 = LSTM(1024, return_sequences=True)(lstm1) \n",
    "    \n",
    "    lstm3 = LSTM(2048, return_sequences=True)(lstm2)\n",
    "    \n",
    "    flat1 = Flatten()(lstm3)\n",
    "    \n",
    "    dense1 = Dense(512)(flat1)\n",
    "    \n",
    "    output = Dense(2, activation='softmax')(dense1)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=r'model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='createmodelRNNR'></a>\n",
    "# RNN ConvR model layout\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block_rnn(rnn_width,X):\n",
    "\n",
    "    # Save the input value. need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = LSTM(rnn_width, return_sequences=True)(X)\n",
    "    # padded_X = tf.keras.preprocessing.sequence.pad_sequences(X,padding='post')\n",
    "    \n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model_R(input_shape):\n",
    "    \n",
    "    # The input to every LSTM layer must be three-dimensional.\n",
    "    visible = Input(input_shape[1:-1])\n",
    "    \n",
    "    lstm1 = LSTM(512, return_sequences=True)(visible)\n",
    "    \n",
    "    X = identity_block_rnn(1024, lstm1) \n",
    "    \n",
    "    X = identity_block_rnn(2048, X)\n",
    "    \n",
    "    flat1 = Flatten()(X)\n",
    "    \n",
    "    dense1 = Dense(512)(flat1)\n",
    "    \n",
    "    output = Dense(2, activation='softmax')(dense1)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=r'model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Confusionmarix'></a>\n",
    "### Confusion marix\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     fontsize=16,\n",
    "                     weight='bold',\n",
    "                     color=\"brown\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     fontsize=16,\n",
    "                     weight='bold',\n",
    "                     color=\"brown\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='filepaths'></a>\n",
    "# Define file paths\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = r\"D:\\EEG\\5202739\"\n",
    "data_path2 = r\"path_1",
    "\n",
    "data_path_list = [data_path1, data_path2]\n",
    "\n",
    "for data_path in data_path_list:\n",
    "        if (path.exists(data_path)):\n",
    "            training_folder_list = loadfiles(data_path + r\"\\Training\\*\")\n",
    "            testing_folder_list = loadfiles(data_path + r\"\\Testing\\*\")\n",
    "            output_file_path = data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_raw_data_single_file(training_folder_list)\n",
    "training_raw_data = read_raw_data_file(output_file_path)\n",
    "raw = band_pass_filter(training_raw_data)\n",
    "event_ids = dict(fatigue_state=1, normal_state=2)\n",
    "training_epochs = epoching(raw, event_ids)\n",
    "\n",
    "ica_epochs, ica = ica_preprocessing(training_epochs)\n",
    "apply_ica(ica_epochs)\n",
    "\n",
    "epochs_copy = ica_epochs.copy()\n",
    "ica.apply(ica_epochs)\n",
    "\n",
    "\n",
    "training_labels_categorical, training_labels = labeling(training_epochs)\n",
    "# print(training_labels)\n",
    "training_epochs.get_data().shape\n",
    "\n",
    "standardized_epochs = z_score(ica_epochs, training_labels)\n",
    "input_shape = standardized_epochs.shape\n",
    "print(\"standardize epochs shape : \",standardized_epochs.shape)\n",
    "print(\"input shape : \",input_shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import modf\n",
    "\n",
    "std_shape = standardized_epochs.shape\n",
    "\n",
    "print(standardized_epochs.reshape(std_shape[0], std_shape[1], std_shape[2]).shape)\n",
    "list1 = np.asarray(standardized_epochs[:,:,:,0]) \n",
    "training_array = []\n",
    "validation_array = []\n",
    "i = 0\n",
    "j = modf(std_shape[0]*2/3)[1]\n",
    "if i < j:\n",
    "    while i < j:\n",
    "        training_array.append(list1[i,:,:])\n",
    "        i = i+1\n",
    "if i >= j:\n",
    "    while i < std_shape[0]:\n",
    "        validation_array.append(list1[i,:,:])\n",
    "        i = i+1\n",
    "        \n",
    "training_array = np.asarray(training_array)\n",
    "validation_array = np.asarray(validation_array)\n",
    "print(training_array.shape)\n",
    "print(validation_array.shape)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['train', 'validation']\n",
    "values = [training_array, validation_array]\n",
    "predintion_dictionary = dict(zip(keys, values))\n",
    "print(predintion_dictionary['validation'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=#fitmodel></a>\n",
    "# Fit the model\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Call backs](#cb)<br>[cnn fit](#cnnfit)<br>[cnn convR fit](#cnnRfit)<br>[rnn fit](#rnnfit)<br>[rnn convR fit](#rnnRfit)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the standardized data into 2d\n",
    "\n",
    "# windows, channels, samples = standardized_epochs[:,:,:,0].shape\n",
    "# feature_vectors_final = standardized_epochs.reshape((windows, channels*samples))\n",
    "\n",
    "feature_vectors_final = standardized_epochs.reshape(len(standardized_epochs[:,:,:,0]),-1)\n",
    "print(\"feature vector final\", feature_vectors_final.shape)\n",
    "print(np.asarray(training_labels).shape)\n",
    "model = SVM_Classify(feature_vectors_final, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range = np.logspace(-6, -1, 5)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    svm.SVC(kernel='rbf', C=2), feature_vectors_final, training_labels, param_name=\"gamma\", param_range=param_range,\n",
    "    scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cb\"></a>\n",
    "#### Callbacks\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelcheckpoint\n",
    "checkpoint_prefix_cnn = \"..\\model_CNN_{epoch}\"\n",
    "checkpoint_prefix_cnn_convR = \"..\\model_CNN_convR_{epoch}\"\n",
    "\n",
    "checkpoint_prefix_rnn = \"..\\model_RNN_{epoch}\"\n",
    "checkpoint_prefix_rnn_convR = \"..\\model_RNN_convR_{epoch}\"\n",
    "\n",
    "# SAVE the weights for the model in the epoch with the lowest val_loss\n",
    "checkpoint_callback_cnn = ModelCheckpoint(filepath=checkpoint_prefix_cnn, save_best_only=True, \n",
    "                                     save_weights_only=True, monitor='val_accuracy', verbose=2)\n",
    "\n",
    "checkpoint_callback_cnn_convR = ModelCheckpoint(filepath=checkpoint_prefix_cnn_convR, save_best_only=True, \n",
    "                                     save_weights_only=True, monitor='val_accuracy', verbose=2)\n",
    "\n",
    "checkpoint_callback_rnn = ModelCheckpoint(filepath=checkpoint_prefix_rnn, save_best_only=True, \n",
    "                                     save_weights_only=True, monitor='val_accuracy', verbose=2)\n",
    "\n",
    "checkpoint_callback_rnn_convR = ModelCheckpoint(filepath=checkpoint_prefix_rnn_convR, save_best_only=True, \n",
    "                                     save_weights_only=True, monitor='val_accuracy', verbose=2)\n",
    "\n",
    "# tensorboard\n",
    "local_time = time.localtime(time.time())\n",
    "logs_folder = \"%s-%s\" % (local_time.tm_mon,local_time.tm_mday)\n",
    "time_format = \"%s.%s\" % (local_time.tm_hour,local_time.tm_min)\n",
    "\n",
    "NAME_CNN = r\"..\\logs\\%s\\logs_cnn_%s\" % (logs_folder,time_format)\n",
    "tensorboard_cnn = TensorBoard(log_dir=NAME_CNN)\n",
    "\n",
    "NAME_CNN_convR = r\"..\\logs\\%s\\logs_cnn_convR_%s\" % (logs_folder,time_format)\n",
    "tensorboard_cnn_convR = TensorBoard(log_dir=NAME_CNN_convR)\n",
    "\n",
    "NAME_RNN =  r\"..\\logs\\%s\\logs_rnn_%s\" % (logs_folder,time_format)\n",
    "tensorboard_rnn = TensorBoard(log_dir=NAME_RNN)\n",
    "\n",
    "NAME_RNN_convR =  r\"..\\logs\\%s\\logs_rnn_convR_%s\" % (logs_folder,time_format)\n",
    "tensorboard_rnn_convR = TensorBoard(log_dir=NAME_RNN_convR)\n",
    "\n",
    "# callbacks array\n",
    "callbacks_cnn = [checkpoint_callback_cnn, tensorboard_cnn]\n",
    "\n",
    "callbacks_cnn_convR = [checkpoint_callback_cnn_convR, tensorboard_cnn_convR]\n",
    "\n",
    "callbacks_rnn = [checkpoint_callback_rnn, tensorboard_rnn]\n",
    "\n",
    "callbacks_rnn_convR = [checkpoint_callback_rnn_convR, tensorboard_rnn_convR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate traning and testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(standardized_epochs, training_labels_categorical, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnnfit\"></a>\n",
    "### CNN model fit\n",
    "<a href=#Top>Top</a><br>\n",
    "<a href=#cb>Call backs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_cnn = create_CNN_model(input_shape).fit(X_train,Y_train, batch_size=10, epochs=30, validation_split=0.1,\n",
    "                    callbacks=callbacks_cnn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_CNN_model(input_shape)\n",
    "latest_model = tf.train.latest_checkpoint(r'..\\CNN_checkpoint')\n",
    "print(latest_model)\n",
    "model.load_weights(latest_model)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('test loss, test acc:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnnRfit\"></a>\n",
    "### CNN_convR model fit\n",
    "<a href=#Top>Top</a><br>\n",
    "<a href=#cb>Call backs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)\n",
    "\n",
    "history_cnn_convR =  create_CNN_model_R(input_shape).fit(X_train,Y_train, batch_size=10, epochs=30, validation_split=0.1,\n",
    "                    callbacks=callbacks_cnn_convR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_CNN_model_R(input_shape)\n",
    "latest_model = tf.train.latest_checkpoint(r'..\\CNN_convR_checkpoInt')\n",
    "print(latest_model)\n",
    "model.load_weights(latest_model)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('test loss, test acc:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=rnnfit></a>\n",
    "### RNN model fit\n",
    "<a href=#Top>Top</a><br>\n",
    "<a href=#cb>Call backs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(np.squeeze(standardized_epochs, axis=3), training_labels_categorical, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rnn = create_RNN_model(input_shape).fit(X_train,Y_train, batch_size=10, epochs=30, validation_split=0.1,\n",
    "                    callbacks=callbacks_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_RNN_model(input_shape)\n",
    "latest_model = tf.train.latest_checkpoint(r'..\\RNN_checkpoInt')\n",
    "print(latest_model)\n",
    "model.load_weights(latest_model)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('test loss, test acc:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=rnnRfit></a>\n",
    "### RNN convR model fit\n",
    "<a href=#Top>Top</a><br>\n",
    "<a href=#cb>Call backs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(np.squeeze(standardized_epochs, axis=3), training_labels_categorical, test_size=0.33)\n",
    "\n",
    "history_rnn = create_RNN_model_R(input_shape).fit(X_train,Y_train, batch_size=20, epochs=20, validation_split=0.1,\n",
    "                    callbacks=callbacks_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.train.latest_checkpoint('..\\modelcheckpoint'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to jason\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_path, weight_path):\n",
    "    # serialize model to JSON\n",
    "    model_CNN = model.to_json()\n",
    "    with open(model_path, \"w\") as json_file:\n",
    "        json_file.write(model_CNN)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weight_path)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"..\\model_CNN.json\"\n",
    "weight_path = r\"..\\model_weights.h5\"\n",
    "save_model(model, model_path, weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loadmodel'></a>\n",
    "# Load model saved in json file\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_path, weight_path):\n",
    "    # load json and create model\n",
    "    from tensorflow.keras.models import model_from_json\n",
    "    json_file = open(model_pathr, 'r')\n",
    "    loaded_model_CNN = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_CNN)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weight_pathr)\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prediction'> </a>\n",
    "# Predictions \n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cnn predict](#cnnpredict)<br>[cnn_convR predict](#cnnRpredict)<br>[rnn predict](#rnnpredict)<br>[svm predict](#svmpredict)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below before <br>\n",
    "[Imports](#imports)<br> [Function List](#funclist)<br> [Apply band pass filter](#bpf)<br> [Create epochs using events](#epoching)<br> [Generate labels array from epochs](#labeling)<br>[Fit ICA to the epochs data](#ica)<br>[Apply ICA to the epochs data](#ica-apply)<br> [Apply standard scala](#sc)<br> [Define file paths](#filepaths)<br> [Load model saved in json file](#loadmodel) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_id = None\n",
    "raw = None\n",
    "data_file_list = []\n",
    "testing_label_list = []\n",
    "pred_class = []\n",
    "\n",
    "for folder in testing_folder_list:\n",
    "   \n",
    "    file_list = loadfiles(folder+\"/*\")\n",
    "    for cnt_file in file_list:\n",
    "        duration= 5\n",
    "\n",
    "        print(os.path.abspath(cnt_file))\n",
    "        \n",
    "        if(os.path.basename(cnt_file) == r\"Fatigue state.cnt\"):\n",
    "            event_id = 1\n",
    "\n",
    "        if(os.path.basename(cnt_file) == r\"Normal state.cnt\"):\n",
    "            event_id = 2\n",
    "        \n",
    "        # Read the file\n",
    "        test_raw = readcnt_noevent(cnt_file, event_id, duration)\n",
    "        \n",
    "        bp_raw = band_pass_filter(test_raw)\n",
    "        \n",
    "        testing_epochs = epoching(bp_raw, event_id)\n",
    "        \n",
    "        #print(event_id,testing_labels)\n",
    "        \n",
    "        ica_testing_epochs, ica = ica_preprocessing(testing_epochs)\n",
    "        \n",
    "        apply_ica(ica_testing_epochs)\n",
    "        \n",
    "        testing_labels_binary, testing_labels = labeling(ica_testing_epochs)\n",
    "        \n",
    "        #print(testing_labels)\n",
    "        \n",
    "        standardized_testing_epochs = z_score(ica_testing_epochs)\n",
    "        \n",
    "        if(len(data_file_list) == 0 and len(testing_label_list) == 0):\n",
    "            data_file_list = standardized_testing_epochs\n",
    "            testing_label_list = testing_labels\n",
    "        else:\n",
    "            data_file_list = np.append(data_file_list, standardized_testing_epochs ,axis=0)\n",
    "            testing_label_list = np.append(testing_labels,testing_label_list)\n",
    "            \n",
    "        print(\"testing data shape \" , data_file_list.shape)\n",
    "        print(np.asarray(testing_label_list).shape)\n",
    "\n",
    "# Create a new model instance\n",
    "input_shape = standardized_testing_epochs.shape\n",
    "\n",
    "\n",
    "#  score = model.evaluate(standardized_testing_epochs, testing_labels_binary, verbose=1)\n",
    "#  print(\"Accuracy: \", score[1])\n",
    "\n",
    "#         # calculate the accuracy \n",
    "#         k = 0\n",
    "#         i = 0\n",
    "#         if(prediction.shape[0] != np.asarray(testing_labels).shape[0]):\n",
    "#             print(\"predicted set size is diffrent from testing labels size\")\n",
    "#         else:\n",
    "#             while(i<prediction.shape[0]):\n",
    "#                 if(prediction[i]==testing_labels[i]):\n",
    "#                     k = k+1\n",
    "#                 i = i+1\n",
    "            \n",
    "                \n",
    "#         print(k, np.asarray(testing_labels).shape[0])                  \n",
    "#         accuracy = k/np.asarray(testing_labels).shape[0]\n",
    "                          \n",
    "#         print(\"Prediction accuracy = \",accuracy)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnnpredict\"></a>\n",
    "### CNN predict\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------  CNN  ---------------------#    \n",
    "\n",
    "model = create_CNN_model(input_shape)\n",
    "# load the latest model saved by model checkpoint with lowest val_loss\n",
    "latest_model = tf.train.latest_checkpoint(r'..\\CNN_checkpoInt')\n",
    "\n",
    "predict_input = data_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnnRpredict\"></a>\n",
    "### CNN convR predict\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------  CNN-convR  ---------------------#    \n",
    "\n",
    "model = create_CNN_model_R(input_shape)\n",
    "# load the latest model saved by model checkpoint with lowest val_loss\n",
    "latest_model = tf.train.latest_checkpoint(r'..\\CNN_convR_checkpoInt')\n",
    "\n",
    "predict_input = data_file_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rnnpredict\"></a>\n",
    "### RNN predict\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------  RNN  ---------------------#\n",
    "\n",
    "model = create_RNN_model(input_shape)\n",
    "latest_model = tf.train.latest_checkpoint(r'..\\RNN_checkpoInt')\n",
    "\n",
    "predict_input = data_file_list[:,:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted class list from CNN and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latest_model)\n",
    "model.load_weights(latest_model)\n",
    "\n",
    "# get the predicted class labesl\n",
    "if(len(pred_class) == 0):\n",
    "    pred_class = model.predict(predict_input).argmax(axis=-1)\n",
    "else:\n",
    "    pred_class = np.append(pred_class,model.predict(predict_input).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svmpredict\"></a>\n",
    "### SVM predict\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = data_file_list.reshape(len(data_file_list[:,:,:,0]),-1)\n",
    "\n",
    "pred_class = model.predict(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(testing_label_list).shape, np.array(pred_class).shape)\n",
    "cm=confusion_matrix(testing_label_list,pred_class,labels=[0, 1])\n",
    "print(cm)\n",
    "plot_confusion_matrix(cm=cm, \n",
    "              normalize    = False,\n",
    "              target_names = ['Normal','Fatigue'],\n",
    "              title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testing_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Accuracy / Loss\n",
    "<a href=#Top>Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.1, 1.5])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# plt.plot(history_rnn.history['loss'], label='loss')\n",
    "# plt.plot(history_rnn.history['val_loss'], label = 'val_loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.ylim([0.0, 3.0])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "plt.plot(model.history['loss'], label='loss')\n",
    "plt.plot(model.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0.0, 3.0])\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
